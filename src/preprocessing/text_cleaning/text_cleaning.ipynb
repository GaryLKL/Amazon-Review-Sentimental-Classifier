{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:24:51.125708Z",
     "start_time": "2020-07-22T02:24:51.106358Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np, pandas as pd\n",
    "import argparse\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from p_tqdm import p_map\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Pool\n",
    "import string\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "en_stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:24:54.289045Z",
     "start_time": "2020-07-22T02:24:54.285329Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    \"data_path\": \"amazon_reviews\"\n",
    "}\n",
    "args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:24:57.012918Z",
     "start_time": "2020-07-22T02:24:57.010981Z"
    }
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--data_path\", help=\"path to where you save the amazon files.\")\n",
    "# args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read Line-delimited JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The Amazon product data](http://jmcauley.ucsd.edu/data/amazon/) is saved as multiple line-delimited json files.\n",
    "\n",
    "I will read all the datasets in one time and then add a \"category\" column for each sample, since the file is stored based on the product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:24:59.963338Z",
     "start_time": "2020-07-22T02:24:59.959289Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_line_json(path, name_list):\n",
    "    json_contents = []\n",
    "    for file_name in name_list:\n",
    "        with open(os.path.join(path, file_name)) as file:\n",
    "            for i, line in enumerate(file):\n",
    "                json_dict = json.loads(line)\n",
    "                json_dict[\"category\"] = file_name[8:-7] # add a column denoting the category\n",
    "                json_contents.append(json_dict)\n",
    "    return json_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:25:02.904365Z",
     "start_time": "2020-07-22T02:25:02.715407Z"
    }
   },
   "outputs": [],
   "source": [
    "amazon_lists = [name for name in os.listdir(args.data_path) if name[-5:] == \".json\"] # ./amazon_reviews\n",
    "\n",
    "''' delete the following line if reading all files '''\n",
    "amazon_lists = [amazon_lists[0]]\n",
    "\n",
    "json_contents = read_line_json(args.data_path, amazon_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. JSON to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the JSON data into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:25:04.600238Z",
     "start_time": "2020-07-22T02:25:04.596934Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_to_df(selected_cols, json_data):\n",
    "    data = pd.DataFrame(json_contents).loc[:, cols]\n",
    "    '1'' Remove duplicated items if existing... '''\n",
    "    # data.sort_values('asin').drop_duplicates(subset=['reviewerID','reviewText','unixReviewTime','summary','category'],keep='first',inplace=False)\n",
    "    ''' Save the DataFrame into a csv file if needed... '''\n",
    "    # data.to_csv()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:25:06.304948Z",
     "start_time": "2020-07-22T02:25:06.267591Z"
    }
   },
   "outputs": [],
   "source": [
    "# The columns I want to keep:\n",
    "cols = [\"reviewerID\", \"asin\", \"reviewText\", \"overall\", \"summary\", \"unixReviewTime\", \"category\"]\n",
    "\n",
    "df = json_to_df(selected_cols=cols, json_data=json_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocess reviewText and sumary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T03:35:41.778109Z",
     "start_time": "2020-07-20T03:35:41.774362Z"
    }
   },
   "source": [
    "Follow the below preprocessing step in order to clean the reviews.\n",
    "\n",
    "1. Remove HTML tags\n",
    "2. Remove url\n",
    "3. Remove emoji\n",
    "4. decontracted\n",
    "5. Remove punctuations and special characters\n",
    "6. Implement Stemming or Lemmatization\n",
    "7. Remove stop words and transform to the lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 NA value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_empty(texts):\n",
    "    # represent NaN as \"\"\n",
    "    if pd.isnull(texts) or texts == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. HTML tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:26:23.180997Z",
     "start_time": "2020-07-22T02:26:23.178272Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_tag(texts):\n",
    "    return BeautifulSoup(texts,'lxml').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:26:25.222443Z",
     "start_time": "2020-07-22T02:26:25.219270Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_url(texts):\n",
    "    return re.sub(r\"http\\S+\", \" \", texts) # \\S non-space characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Emoji\n",
    "\n",
    "Remove Emoji if any exists...\n",
    "\n",
    "Note: Emoji could be used to recognize the sentiment, potentially..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:26:27.225280Z",
     "start_time": "2020-07-22T02:26:27.221666Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_emoji(texts):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:26:29.213508Z",
     "start_time": "2020-07-22T02:26:29.210033Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_emoji(texts):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return bool(re.match(emoji_pattern, texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Contraction\n",
    "\n",
    "Example: I've -> I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:26:31.185459Z",
     "start_time": "2020-07-22T02:26:31.182523Z"
    }
   },
   "outputs": [],
   "source": [
    "def decontracted(texts):\n",
    "    return contractions.fix(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Punctuation; Speical Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:26:33.286054Z",
     "start_time": "2020-07-22T02:26:33.281881Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "def remove_punc(texts):\n",
    "    return \" \".join(\"\".join([\" \" if ch in punctuations or not ch.isalpha() else ch for ch in texts]).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Stemming or Lemmatization\n",
    "\n",
    "POS_tag is helpful for using text lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:26:35.266172Z",
     "start_time": "2020-07-22T02:26:35.262473Z"
    }
   },
   "outputs": [],
   "source": [
    "# POS_tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:26:37.135355Z",
     "start_time": "2020-07-22T02:26:37.131406Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_lemma(texts):\n",
    "    # tokenized and pos tag\n",
    "    tokens = word_tokenize(texts)\n",
    "    tagged_sent = pos_tag(tokens)\n",
    "    \n",
    "    # lemma\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos))\n",
    "\n",
    "    return lemmas_sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Lower Case; Stop Words\n",
    "\n",
    "Considering some negative words which could affect the sentiment analysis, I would like to remove them from the stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:27:43.023084Z",
     "start_time": "2020-07-22T02:27:43.020200Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_set = {\"no\", \"nor\", \"not\"}\n",
    "my_en_stop_words = {w for w in en_stop_words if w not in neg_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:44:22.672709Z",
     "start_time": "2020-07-22T02:44:22.669481Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    # remove stop words and then transform to lower case\n",
    "    return [w.lower() for w in tokens if w.lower() not in my_en_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Preprocessing Function\n",
    "\n",
    "Let's integrate all the functions above to extract cleaned tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:44:24.489492Z",
     "start_time": "2020-07-22T02:44:24.486213Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_step_preprocessing(texts):\n",
    "    # this function will do preprocessing on a string and then return a list of tokens\n",
    "    token_result = clean_tokens(\n",
    "                        to_lemma(\n",
    "                            remove_punc(\n",
    "                                decontracted(\n",
    "                                    remove_emoji(\n",
    "                                        remove_url(\n",
    "                                            remove_tag(\n",
    "                                                to_empty(\n",
    "                                                    texts\n",
    "                                                )\n",
    "                                            )\n",
    "                                        )\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "    return token_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocessing(texts):\n",
    "    # this function will do preprocessing on a string and then return a list of tokens\n",
    "    token_result = decontracted(\n",
    "                        remove_emoji(\n",
    "                            remove_url(\n",
    "                                remove_tag(\n",
    "                                    to_empty(\n",
    "                                        texts\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "    return token_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Preprocessing with multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:44:46.083420Z",
     "start_time": "2020-07-22T02:44:26.241394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56e76b2e56c443f88128fefd6db97ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"reviewTokens\"] = p_map(preprocess_func, df[\"reviewText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T02:44:48.097162Z",
     "start_time": "2020-07-22T02:44:47.947044Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_json(os.path.join(args.data_path, \"amazon_reviews.json\"), orient=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
