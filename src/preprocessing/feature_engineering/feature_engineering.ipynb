{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages & Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:00.417674Z",
     "start_time": "2020-08-04T02:21:00.414725Z"
    }
   },
   "outputs": [],
   "source": [
    "# set root\n",
    "import os\n",
    "os.chdir(\"/scratch/kll482/cathay/\")\n",
    "import sys\n",
    "sys.path.append(\"/scratch/kll482/cathay/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.778210Z",
     "start_time": "2020-08-04T02:21:00.419907Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from configparser import ConfigParser\n",
    "import numpy as np, pandas as pd\n",
    "import time\n",
    "from p_tqdm import p_map\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import torch\n",
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "''' customized modules '''\n",
    "from src.preprocessing.text_cleaning.text_cleaning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.782999Z",
     "start_time": "2020-08-04T02:21:01.780888Z"
    }
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# parser = {\n",
    "#     \"data_path\": \"amazon_reviews\",\n",
    "# }\n",
    "# args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.788816Z",
     "start_time": "2020-08-04T02:21:01.785056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text_cleaning', 'feature_engineering', 'graph_models']\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser()\n",
    "config.read(\"config/config.ini\")\n",
    "print(config.sections())\n",
    "args = config[\"feature_engineering\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Read Line-delimited JSON\n",
    "\n",
    "[The Amazon product data](http://jmcauley.ucsd.edu/data/amazon/) is saved as multiple line-delimited json files.\n",
    "\n",
    "I will read all the datasets in one time and then add a \"category\" column for each sample, since the file is stored based on the product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.794800Z",
     "start_time": "2020-08-04T02:21:01.790552Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_line_json(path, name_list):\n",
    "    json_contents = []\n",
    "    for file_name in name_list:\n",
    "        with open(os.path.join(path, file_name)) as file:\n",
    "            for i, line in enumerate(file):\n",
    "                json_dict = json.loads(line)\n",
    "                json_dict[\"category\"] = file_name[8:-7] # add a column denoting the category\n",
    "                json_contents.append(json_dict)\n",
    "    return json_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.947073Z",
     "start_time": "2020-08-04T02:21:01.796588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Video_Games_5.json', 'Musical_Instruments_5.json']\n"
     ]
    }
   ],
   "source": [
    "folder_path = args[\"amazon_file_path\"]\n",
    "file_lists = [name for name in os.listdir(folder_path) if name[-5:] == \".json\"] # ./amazon_reviews\n",
    "print(file_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' delete the next line if reading all files '''\n",
    "# file_lists = [file_lists[1]]\n",
    "json_contents = read_line_json(folder_path, file_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T03:17:06.387335Z",
     "start_time": "2020-07-23T03:17:06.383553Z"
    }
   },
   "source": [
    "Let's convert data in JSON format to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.951965Z",
     "start_time": "2020-08-04T02:21:01.948960Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_to_df(json_data, selected_cols=None):\n",
    "    if selected_cols is None:\n",
    "        data = pd.DataFrame(json_contents)\n",
    "    else:\n",
    "        data = pd.DataFrame(json_contents).loc[:, cols]\n",
    "    '1'' Remove duplicated items if existing... '''\n",
    "    # data.sort_values('asin').drop_duplicates(subset=['reviewerID','reviewText','unixReviewTime','summary','category'],keep='first',inplace=False)\n",
    "    ''' Save the DataFrame into a csv file if needed... '''\n",
    "    # data.to_csv()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = json_to_df(json_data=json_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:02.001135Z",
     "start_time": "2020-08-04T02:21:01.955748Z"
    }
   },
   "outputs": [],
   "source": [
    "# The columns I want to keep:\n",
    "cols = [\"reviewerID\", \"asin\", \"reviewText\", \"overall\", \"summary\", \"unixReviewTime\", \"category\"]\n",
    "df = json_to_df(json_data=json_contents, selected_cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728969\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/swvgdtt/your-account/manage-downloads.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B00KWHOXGI?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_2\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B00KWHJ1O2?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B00KWHJ0KC?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_5\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B00NXUKGQC?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_2\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B003TN088Y/ref=cm_cr_ryp_prd_ttl_sol_43\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B0002E1G5C?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_51\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B0007XTOCA/ref=cm_cr_ryp_prd_ttl_sol_25\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B001MXG49U/ref=cm_cr_ryp_prd_ttl_sol_10\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B001XJBWXG?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_49\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "[i for i, v in enumerate(df[\"reviewText\"]) if not pd.isnull(v) and len(remove_tag(v)) != len(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i, v in enumerate(df[\"reviewText\"]) if not pd.isnull(v) and len(remove_tag(v)) != len(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Preprocessing Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we should do some text cleaning first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in enumerate(df[\"reviewText\"]):\n",
    "#     result = []\n",
    "#     try:\n",
    "#         result.append(full_step_preprocessing(row))\n",
    "#     except:\n",
    "#         print(idx)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_tokens(df, review_col_name=\"reviewText\", token_col_name=\"reviewTokens\", cpu_number=4):\n",
    "    pool = mp.Pool(cpu_number)\n",
    "    df[token_col_name] = pool.map(full_step_preprocessing, tqdm(df[review_col_name]))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_tokens(df, token_col_name=\"reviewTokens\"):\n",
    "    empty_row_index = list(df[token_col_name][df[token_col_name].apply(lambda x: len(x)==0)].index)\n",
    "    df = df.drop(axis=0, index=empty_row_index).reset_index(drop=True)\n",
    "    \n",
    "    assert sum(df[token_col_name].apply(lambda x: len(x)==0)) == 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.667403Z",
     "start_time": "2020-08-04T02:21:02.004358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e27d9bee344b00bf8c28f763138e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=728969.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/swvgdtt/your-account/manage-downloads.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B00KWHOXGI?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_2\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B00KWHJ1O2?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B00KWHJ0KC?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_5\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B003TN088Y/ref=cm_cr_ryp_prd_ttl_sol_43\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B0002E1G5C?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_51\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B00NXUKGQC?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_2\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B0007XTOCA/ref=cm_cr_ryp_prd_ttl_sol_25\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B001XJBWXG?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_49\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B00Y4S5Z5O/ref=cm_cr_ryp_prd_ttl_sol_46\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B009YT5ZZS/ref=cm_cr_ryp_prd_ttl_sol_41\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B009MBT68U/ref=cm_cr_ryp_prd_ttl_sol_14\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B001MXG49U/ref=cm_cr_ryp_prd_ttl_sol_10\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B004QM8YRE/ref=cm_cr_ryp_prd_ttl_sol_35\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B000BKY8CU/ref=cm_cr_ryp_prd_ttl_sol_27\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B001MZW546/ref=cm_cr_ryp_prd_ttl_sol_8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B00F9DQXSG/ref=cm_cr_ryp_prd_ttl_sol_6\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B004XJNIWG/ref=cm_cr_ryp_prd_ttl_sol_11\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "df = get_cleaned_tokens(df, \"reviewText\", \"reviewTokens\", cpu_number=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.705989Z",
     "start_time": "2020-08-04T02:21:28.676251Z"
    }
   },
   "outputs": [],
   "source": [
    "df = remove_empty_tokens(df, \"reviewTokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Edge Index & Unique Tokens\n",
    "\n",
    "The function is to convert tokens of a reivews to an adjacency matrix based on n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.710705Z",
     "start_time": "2020-08-04T02:21:28.707819Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_adjacency_matrix(tokens, num_neighbor=2):\n",
    "#     # initialize\n",
    "#     unique_vocabulary = set(tokens) \n",
    "#     vocabulary_dict = {value: index for index, value in enumerate(unique_vocabulary)}\n",
    "#     width = height = len(unique_vocabulary)\n",
    "#     adjacency_matrix = [[0]*width for _ in range(height)]\n",
    "    \n",
    "#     edge_start = []\n",
    "#     edge_end = []\n",
    "#     # insert value into the adjacency matrix\n",
    "#     for token_index, token in enumerate(tokens):\n",
    "#         matrix_index = vocabulary_dict[token]\n",
    "#         for p in range(1, num_neighbor+1):\n",
    "#             if token_index-p >= 0: # if previous tokens exist\n",
    "#                 prev_matrix_index = vocabulary_dict[tokens[token_index-p]]\n",
    "#                 adjacency_matrix[matrix_index][prev_matrix_index] = 1 # future work: if duplicated edges exist...\n",
    "#                 adjacency_matrix[prev_matrix_index][matrix_index] = 1\n",
    "                \n",
    "#                 edge_start += [matrix_index, prev_matrix_index]\n",
    "#                 edge_end += [prev_matrix_index, matrix_index]\n",
    "                \n",
    "#             elif token_index+p < len(tokens): # if next tokes exist\n",
    "#                 next_matrix_index = vocabulary_dict[tokens[token_index+p]] # get the token index in the adjacency matrix\n",
    "#                 adjacency_matrix[matrix_index][next_matrix_index] = 1\n",
    "#                 adjacency_matrix[next_matrix_index][matrix_index] = 1\n",
    "                \n",
    "#                 edge_start += [matrix_index, next_matrix_index]\n",
    "#                 edge_end += [next_matrix_index, matrix_index]\n",
    "    \n",
    "#     unique_tokens = list(vocabulary_dict.keys())\n",
    "#     edge_index = [edge_start, edge_end]\n",
    "#     return adjacency_matrix, unique_tokens, edge_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.721249Z",
     "start_time": "2020-08-04T02:21:28.713296Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_edge_index(tokens, unique_vocabulary, num_neighbor=2, bidirection=True):\n",
    "    # initialize\n",
    "#     unique_vocabulary = set(tokens) \n",
    "    vocabulary_dict = {value: index for index, value in enumerate(unique_vocabulary)} # dictionary of unique tokens\n",
    "    edge_start = []\n",
    "    edge_end = []\n",
    "    \n",
    "    # build edge index\n",
    "    for token_index, token in enumerate(tokens):\n",
    "        curr_index = vocabulary_dict[token] # current token's index in vocabulary_dict\n",
    "        \n",
    "        for p in range(1, num_neighbor+1): # find neighbors of current tokens\n",
    "            if bidirection == True: # bi-direction\n",
    "                if token_index-p >= 0: # if previous p token exists\n",
    "                    prev_index = vocabulary_dict[tokens[token_index-p]] # get the index of the previous p token\n",
    "                    edge_start += [curr_index, prev_index] # undirected\n",
    "                    edge_end += [prev_index, curr_index]\n",
    "                \n",
    "            if token_index+p < len(tokens): # if next p toke exists\n",
    "                next_index = vocabulary_dict[tokens[token_index+p]] # get the index of the next p token   \n",
    "                edge_start += [curr_index, next_index]\n",
    "                edge_end += [next_index, curr_index]\n",
    "    \n",
    "    edge_index = [edge_start, edge_end]\n",
    "    return edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique tokens\n",
    "df[\"uniqueTokens\"] = df[\"reviewTokens\"].apply(lambda row: list(set(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:31.162657Z",
     "start_time": "2020-08-04T02:21:28.723255Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d11a7e0a0ab446ca8f8b2e39dc0db1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# edge index\n",
    "edge_index_names = []\n",
    "for neighbor in tqdm([1]):\n",
    "    # get edge index with n neighbors\n",
    "    edge_index = df.apply(lambda row: get_edge_index(tokens=row[\"reviewTokens\"],\n",
    "                                                     unique_vocabulary=row[\"uniqueTokens\"],\n",
    "                                                     num_neighbor=neighbor,\n",
    "                                                     bidirection=False\n",
    "                                                    ),\n",
    "                          axis=1\n",
    "                         )\n",
    "    df[\"edgeIndex{}\".format(neighbor)] = edge_index # insert edge indices to the dataframe\n",
    "    edge_index_names.append(\"edgeIndex{}\".format(neighbor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_names = [\"edgeIndex2\", \"edgeIndex3\", \"edgeIndex4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"overall\", \"reviewTokens\", \"uniqueTokens\"]+edge_index_names].to_json(\"dataset/full_dataset/partial_features.json\", orient=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== preprocessing file Musical_Instruments_5.json ===\n",
      "json to dataframe...\n",
      "cleaning texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e192ea561edc416cb7cc8816460e8c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=231392.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B0002E1G5C?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_51\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B0007XTOCA/ref=cm_cr_ryp_prd_ttl_sol_25\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B003TN088Y/ref=cm_cr_ryp_prd_ttl_sol_43\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B001MXG49U/ref=cm_cr_ryp_prd_ttl_sol_10\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B001XJBWXG?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_49\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B004XJNIWG/ref=cm_cr_ryp_prd_ttl_sol_11\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B009MBT68U/ref=cm_cr_ryp_prd_ttl_sol_14\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B00F9DQXSG/ref=cm_cr_ryp_prd_ttl_sol_6\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B000BKY8CU/ref=cm_cr_ryp_prd_ttl_sol_27\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B00Y4S5Z5O/ref=cm_cr_ryp_prd_ttl_sol_46\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B004QM8YRE/ref=cm_cr_ryp_prd_ttl_sol_35\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B009YT5ZZS/ref=cm_cr_ryp_prd_ttl_sol_41\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.amazon.com/gp/product/B001MZW546/ref=cm_cr_ryp_prd_ttl_sol_8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract unique tokens...\n",
      "getting the edge index for each graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages/ipykernel_launcher.py:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536f1329b3114df7a1191132f31ed195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving jsoin files...\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "''' 2. handle and save each file respectively '''\n",
    "CPU_COUNT = 28\n",
    "for file in [file_lists[1]]:\n",
    "    print(\"=== preprocessing file {} ===\".format(file))\n",
    "    ''' 2-1. json to dataframe '''\n",
    "    print(\"json to dataframe...\")\n",
    "    json_contents = read_line_json(folder_path, [file]) # reading the json files\n",
    "    cols = [\"reviewerID\", \"asin\", \"reviewText\", \"overall\", \"summary\", \"unixReviewTime\", \"category\"] # the columns I want to keep:\n",
    "    df = json_to_df(json_data=json_contents, selected_cols=cols)\n",
    "\n",
    "    ''' 2-2. do text cleaning '''\n",
    "    print(\"cleaning texts...\")\n",
    "    df = get_cleaned_tokens(df, \"reviewText\", \"reviewTokens\", cpu_number=CPU_COUNT)\n",
    "    df = remove_empty_tokens(df, \"reviewTokens\")\n",
    "\n",
    "    ''' 2-3. get unique tokens from review tokens '''\n",
    "    print(\"extract unique tokens...\")\n",
    "    df[\"uniqueTokens\"] = df[\"reviewTokens\"].apply(lambda row: list(set(row)))\n",
    "\n",
    "    ''' 2-4. get edge index from unique tokens '''\n",
    "    print(\"getting the edge index for each graph...\")\n",
    "    # edge index\n",
    "    edge_index_names = []\n",
    "    neighbors = [1]\n",
    "    bidirection_or_not = False\n",
    "    for neighbor in tqdm(neighbors):\n",
    "        # get edge index with n neighbors\n",
    "        edge_index = df.apply(lambda row: get_edge_index(tokens=row[\"reviewTokens\"],\n",
    "                                                         unique_vocabulary=row[\"uniqueTokens\"],\n",
    "                                                         num_neighbor=neighbor,\n",
    "                                                         bidirection=bidirection_or_not,\n",
    "                                                        ),\n",
    "                              axis=1\n",
    "                             )\n",
    "        df[\"edgeIndex{}\".format(neighbor)] = edge_index # insert edge indices to the dataframe\n",
    "        edge_index_names.append(\"edgeIndex{}\".format(neighbor))\n",
    "        \n",
    "    ''' 2-5. save df as a new json file '''\n",
    "    print(\"saving jsoin files...\")\n",
    "    edge_index_names = [\"edgeIndex{}\".format(n) for n in neighbors]\n",
    "    df.loc[:, [\"overall\", \"reviewText\", \"reviewTokens\", \"uniqueTokens\"]+edge_index_names].to_json(\"dataset/processed_dataset/{}\".format(\"straight_\"+file), orient=\"columns\")\n",
    "    print(\"finish!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Token Length Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:31.517492Z",
     "start_time": "2020-08-04T02:21:31.515076Z"
    }
   },
   "outputs": [],
   "source": [
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:31.521843Z",
     "start_time": "2020-08-04T02:21:31.519581Z"
    }
   },
   "outputs": [],
   "source": [
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. BERT Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T03:00:16.220334Z",
     "start_time": "2020-08-04T02:59:57.104Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertEmbedding:\n",
    "    def __init__(self, max_len=None):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.embedding_matrix = self.get_bert_embed_matrix()\n",
    "        \n",
    "    def get_bert_embed_matrix(self):\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        bert_embeddings = list(bert.children())[0]\n",
    "        bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "        mat = bert_word_embeddings.weight.data.numpy()\n",
    "        return mat\n",
    "\n",
    "    def get_embeddings(self, row_data):\n",
    "        '''\n",
    "        @ param, row_data: a unique token list\n",
    "        '''\n",
    "        if len(row_data) == 0:\n",
    "            return []\n",
    "        \n",
    "        if self.max_len is None:\n",
    "            MAX_LEN = len(row_data)+2 # +2 is for adding cls and \\cls\n",
    "        else:\n",
    "            MAX_LEN = self.max_len\n",
    "            \n",
    "        input_ids = self.tokenizer.encode(row_data,\n",
    "                                          max_length=MAX_LEN,\n",
    "                                          truncation=True,\n",
    "                                          pad_to_max_length=True\n",
    "                                         )\n",
    "        input_ids = input_ids[1:-1] # however, we do not take cls & \\cls into consideration when building the embeddings\n",
    "        embeddings = []\n",
    "        for index in input_ids:\n",
    "            embeddings.append(self.embedding_matrix[index])\n",
    "\n",
    "        assert len(embeddings) == len(input_ids) and len(embeddings[0]) == self.embedding_matrix.shape[1]\n",
    "        return embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T08:01:14.731820Z",
     "start_time": "2020-07-24T08:01:14.728762Z"
    }
   },
   "source": [
    "First, I will create a BERT embedding matrix for unique word list which will be used in Graph models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = df[\"uniqueTokens\"].values.tolist()\n",
    "bertembeddings = BertEmbedding()\n",
    "df[\"graphEmbeddings\"] = [bertembeddings.get_embeddings(row) for row in tqdm(unique_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:29:45.921264Z",
     "start_time": "2020-08-04T02:28:22.639252Z"
    }
   },
   "outputs": [],
   "source": [
    "# # initial\n",
    "# with Pool(8) as p:\n",
    "#     unique_tokens = df[\"uniqueTokens\"].values.tolist()\n",
    "# #     MAX_LEN = int(np.median([len(row) for row in df[\"uniqueTokens\"]])) # median_unique_word_length\n",
    "# #     bertembeddings = BertEmbedding(MAX_LEN)\n",
    "#     bertembeddings = BertEmbedding()\n",
    "#     df[\"graphEmbeddings\"] = p.map(bertembeddings.get_embeddings, tqdm_notebook(unique_tokens))\n",
    "\n",
    "# # df[\"graphEmbeddings\"] = graph_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, I will also create another embedding matrix for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:04:41.553671Z",
     "start_time": "2020-08-04T02:04:30.031Z"
    }
   },
   "outputs": [],
   "source": [
    "with Pool(8) as p:\n",
    "    review_tokens = df[\"reviewTokens\"].values.tolist()\n",
    "#     MAX_LEN = int(np.median([len(row) for row in df[\"reviewTokens\"]])) # median_review_length\n",
    "#     bertembeddings = BertEmbedding(MAX_LEN)\n",
    "    bertembeddings = BertEmbedding()\n",
    "    language_embeddings = p.map(bertembeddings.get_embeddings, tqdm(review_tokens))\n",
    "    p.terminate()\n",
    "    \n",
    "df[\"languageEmbeddings\"] = language_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Random Embedding\n",
    "\n",
    "PyTorch will automatically generate random embeddings for us if we do not insert embeddings into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T03:04:20.500846Z",
     "start_time": "2020-08-04T03:04:09.686318Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.loc[:, [\"overall\", \"edgeIndex\", \"graphEmbeddings\"]].to_pickle(\"dataset/full_dataset/modeling_features.pkl\")\n",
    "df.loc[:, [\"overall\", \"edgeIndex\", \"graphEmbeddings\"]].to_json(args[\"modeling_feature_path\"], orient=\"columns\")\n",
    "# df.to_pickle(\n",
    "#     os.path.join(\n",
    "#         args[\"data_path\"],\n",
    "#         \"amazon_features.pkl\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-2.3.2-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy) (49.2.0.post20200714)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy) (4.48.0)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.7.1.tar.gz (22 kB)\n",
      "Collecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 119.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 121.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (185 kB)\n",
      "\u001b[K     |████████████████████████████████| 185 kB 122.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy) (1.18.5)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl (32 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 98.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (19 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
      "Building wheels for collected packages: wasabi\n",
      "  Building wheel for wasabi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wasabi: filename=wasabi-0.7.1-py3-none-any.whl size=20834 sha256=218403e1d3a643c3331ec0fad4893900ca986d4e5f9c58d0cb1a022969b1691f\n",
      "  Stored in directory: /home/kll482/.cache/pip/wheels/dc/5e/d4/727b6213e9ebec502ff1bf5998f4a83fef87c3aace8a492243\n",
      "Successfully built wasabi\n",
      "Installing collected packages: catalogue, plac, wasabi, cymem, murmurhash, preshed, srsly, blis, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 wasabi-0.7.1\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.48.0)\n",
      "Requirement already satisfied: setuptools in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /scratch/kll482/anaconda3/envs/cathay/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=8fae690aa23dbea9b082081d147d1430c457226bbb0c99dd16635fea3eae4598\n",
      "  Stored in directory: /state/partition1/job-12016132/pip-ephem-wheel-cache-beq4xcfb/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
