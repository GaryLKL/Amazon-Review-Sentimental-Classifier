{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages & Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:00.417674Z",
     "start_time": "2020-08-04T02:21:00.414725Z"
    }
   },
   "outputs": [],
   "source": [
    "# set root\n",
    "import os\n",
    "os.chdir(\"/scratch/kll482/cathay/\")\n",
    "import sys\n",
    "sys.path.append(\"/scratch/kll482/cathay/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.778210Z",
     "start_time": "2020-08-04T02:21:00.419907Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from configparser import ConfigParser\n",
    "import numpy as np, pandas as pd\n",
    "import time\n",
    "from p_tqdm import p_map\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import torch\n",
    "\n",
    "''' customized modules '''\n",
    "from src.preprocessing.text_cleaning.text_cleaning import full_step_preprocessing, simple_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.782999Z",
     "start_time": "2020-08-04T02:21:01.780888Z"
    }
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# parser = {\n",
    "#     \"data_path\": \"amazon_reviews\",\n",
    "# }\n",
    "# args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.788816Z",
     "start_time": "2020-08-04T02:21:01.785056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text_cleaning', 'feature_engineering', 'graph_models']\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser()\n",
    "config.read(\"config/config.ini\")\n",
    "print(config.sections())\n",
    "args = config[\"feature_engineering\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Line-delimited JSON\n",
    "\n",
    "[The Amazon product data](http://jmcauley.ucsd.edu/data/amazon/) is saved as multiple line-delimited json files.\n",
    "\n",
    "I will read all the datasets in one time and then add a \"category\" column for each sample, since the file is stored based on the product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.794800Z",
     "start_time": "2020-08-04T02:21:01.790552Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_line_json(path, name_list):\n",
    "    json_contents = []\n",
    "    for file_name in name_list:\n",
    "        with open(os.path.join(path, file_name)) as file:\n",
    "            for i, line in enumerate(file):\n",
    "                json_dict = json.loads(line)\n",
    "                json_dict[\"category\"] = file_name[8:-7] # add a column denoting the category\n",
    "                json_contents.append(json_dict)\n",
    "    return json_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.947073Z",
     "start_time": "2020-08-04T02:21:01.796588Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_path = args[\"amazon_file_path\"]\n",
    "file_lists = [name for name in os.listdir(folder_path) if name[-5:] == \".json\"] # ./amazon_reviews\n",
    "\n",
    "''' delete the next line if reading all files '''\n",
    "file_lists = [file_lists[0]]\n",
    "json_contents = read_line_json(folder_path, file_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T03:17:06.387335Z",
     "start_time": "2020-07-23T03:17:06.383553Z"
    }
   },
   "source": [
    "Let's convert data in JSON format to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:01.951965Z",
     "start_time": "2020-08-04T02:21:01.948960Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_to_df(selected_cols, json_data):\n",
    "    data = pd.DataFrame(json_contents).loc[:, cols]\n",
    "    '1'' Remove duplicated items if existing... '''\n",
    "    # data.sort_values('asin').drop_duplicates(subset=['reviewerID','reviewText','unixReviewTime','summary','category'],keep='first',inplace=False)\n",
    "    ''' Save the DataFrame into a csv file if needed... '''\n",
    "    # data.to_csv()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:02.001135Z",
     "start_time": "2020-08-04T02:21:01.955748Z"
    }
   },
   "outputs": [],
   "source": [
    "# The columns I want to keep:\n",
    "cols = [\"reviewerID\", \"asin\", \"reviewText\", \"overall\", \"summary\", \"unixReviewTime\", \"category\"]\n",
    "df = json_to_df(selected_cols=cols, json_data=json_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we should do some text cleaning first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_tokens(df, review_col_name=\"reviewText\", token_col_name=\"reviewTokens\", cpu_number=4):\n",
    "    print(\"cleaning the reviews...\")\n",
    "    pool = mp.Pool(cpu_number)\n",
    "    df[token_col_name] = pool.map(full_step_preprocessing, tqdm_notebook(df[review_col_name]))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.667403Z",
     "start_time": "2020-08-04T02:21:02.004358Z"
    }
   },
   "outputs": [],
   "source": [
    "df = get_cleaned_tokens(df, \"reviewText\", \"reviewTokens\", cpu_number=20)\n",
    "\n",
    "# with open(os.path.join(args.data_path, args.file_name), \"r+\") as file:\n",
    "#     json_data = json.load(file)\n",
    "# df = pd.DataFrame(json_data)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.674266Z",
     "start_time": "2020-08-04T02:21:28.669379Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_empty_tokens(df, token_col_name=\"reviewTokens\"):\n",
    "    empty_row_index = list(df[token_col_name][df[token_col_name].apply(lambda x: len(x)==0)].index)\n",
    "    df = df.drop(axis=0, index=empty_row_index).reset_index(drop=True)\n",
    "    \n",
    "    assert sum(df[token_col_name].apply(lambda x: len(x)==0)) == 0\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.705989Z",
     "start_time": "2020-08-04T02:21:28.676251Z"
    }
   },
   "outputs": [],
   "source": [
    "df = remove_empty_tokens(df, \"reviewTokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adjacency & Edge Index\n",
    "\n",
    "The function is to convert tokens of a reivews to an adjacency matrix based on n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.710705Z",
     "start_time": "2020-08-04T02:21:28.707819Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_adjacency_matrix(tokens, num_neighbor=2):\n",
    "#     # initialize\n",
    "#     unique_vocabulary = set(tokens) \n",
    "#     vocabulary_dict = {value: index for index, value in enumerate(unique_vocabulary)}\n",
    "#     width = height = len(unique_vocabulary)\n",
    "#     adjacency_matrix = [[0]*width for _ in range(height)]\n",
    "    \n",
    "#     edge_start = []\n",
    "#     edge_end = []\n",
    "#     # insert value into the adjacency matrix\n",
    "#     for token_index, token in enumerate(tokens):\n",
    "#         matrix_index = vocabulary_dict[token]\n",
    "#         for p in range(1, num_neighbor+1):\n",
    "#             if token_index-p >= 0: # if previous tokens exist\n",
    "#                 prev_matrix_index = vocabulary_dict[tokens[token_index-p]]\n",
    "#                 adjacency_matrix[matrix_index][prev_matrix_index] = 1 # future work: if duplicated edges exist...\n",
    "#                 adjacency_matrix[prev_matrix_index][matrix_index] = 1\n",
    "                \n",
    "#                 edge_start += [matrix_index, prev_matrix_index]\n",
    "#                 edge_end += [prev_matrix_index, matrix_index]\n",
    "                \n",
    "#             elif token_index+p < len(tokens): # if next tokes exist\n",
    "#                 next_matrix_index = vocabulary_dict[tokens[token_index+p]] # get the token index in the adjacency matrix\n",
    "#                 adjacency_matrix[matrix_index][next_matrix_index] = 1\n",
    "#                 adjacency_matrix[next_matrix_index][matrix_index] = 1\n",
    "                \n",
    "#                 edge_start += [matrix_index, next_matrix_index]\n",
    "#                 edge_end += [next_matrix_index, matrix_index]\n",
    "    \n",
    "#     unique_tokens = list(vocabulary_dict.keys())\n",
    "#     edge_index = [edge_start, edge_end]\n",
    "#     return adjacency_matrix, unique_tokens, edge_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:28.721249Z",
     "start_time": "2020-08-04T02:21:28.713296Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_edge_index(tokens, num_neighbor=2):\n",
    "    # initialize\n",
    "    unique_vocabulary = set(tokens) \n",
    "    vocabulary_dict = {value: index for index, value in enumerate(unique_vocabulary)} # dictionary of unique tokens\n",
    "    edge_start = []\n",
    "    edge_end = []\n",
    "    \n",
    "    # build edge index\n",
    "    for token_index, token in enumerate(tokens):\n",
    "        curr_index = vocabulary_dict[token] # current token's index in vocabulary_dict\n",
    "        \n",
    "        for p in range(1, num_neighbor+1): # find neighbors of current tokens\n",
    "            if token_index-p >= 0: # if previous p token exists\n",
    "                prev_index = vocabulary_dict[tokens[token_index-p]] # get the index of the previous p token\n",
    "                edge_start += [curr_index, prev_index] # undirected\n",
    "                edge_end += [prev_index, curr_index]\n",
    "                \n",
    "            if token_index+p < len(tokens): # if next p toke exists\n",
    "                next_index = vocabulary_dict[tokens[token_index+p]] # get the index of the next p token   \n",
    "                edge_start += [curr_index, next_index]\n",
    "                edge_end += [next_index, curr_index]\n",
    "    \n",
    "    unique_tokens = list(vocabulary_dict.keys())\n",
    "    edge_index = [edge_start, edge_end]\n",
    "    return edge_index, unique_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:31.162657Z",
     "start_time": "2020-08-04T02:21:28.723255Z"
    }
   },
   "outputs": [],
   "source": [
    "# edge index\n",
    "num_neighbor = 2\n",
    "edge_index_info = df[\"reviewTokens\"].apply(lambda row: get_edge_index(row, num_neighbor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:31.512760Z",
     "start_time": "2020-08-04T02:21:31.164571Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. insert edge index to the dataframe\n",
    "df[\"edgeIndex\"] = [row[0] for row in edge_index_info]\n",
    "\n",
    "# 2. insert unique token to the dataframe\n",
    "df[\"uniqueTokens\"] = [row[1] for row in edge_index_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Token Length Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:31.517492Z",
     "start_time": "2020-08-04T02:21:31.515076Z"
    }
   },
   "outputs": [],
   "source": [
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:21:31.521843Z",
     "start_time": "2020-08-04T02:21:31.519581Z"
    }
   },
   "outputs": [],
   "source": [
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. BERT Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T03:00:16.220334Z",
     "start_time": "2020-08-04T02:59:57.104Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertEmbedding:\n",
    "    def __init__(self, max_len=None):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.embedding_matrix = self.get_bert_embed_matrix()\n",
    "        \n",
    "    def get_bert_embed_matrix(self):\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        bert_embeddings = list(bert.children())[0]\n",
    "        bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "        mat = bert_word_embeddings.weight.data.numpy()\n",
    "        return mat\n",
    "\n",
    "    def get_embeddings(self, row_data):\n",
    "        '''\n",
    "        @ param, row_data: a unique token list\n",
    "        '''\n",
    "        if len(row_data) == 0:\n",
    "            return []\n",
    "        \n",
    "        if self.max_len is None:\n",
    "            MAX_LEN = len(row_data)+2 # +2 is for adding cls and \\cls\n",
    "        else:\n",
    "            MAX_LEN = self.max_len\n",
    "            \n",
    "        input_ids = self.tokenizer.encode(row_data,\n",
    "                                          max_length=MAX_LEN,\n",
    "                                          truncation=True,\n",
    "                                          pad_to_max_length=True\n",
    "                                         )\n",
    "        input_ids = input_ids[1:-1] # however, we do not take cls & \\cls into consideration when building the embeddings\n",
    "        embeddings = []\n",
    "        for index in input_ids:\n",
    "            embeddings.append(self.embedding_matrix[index])\n",
    "\n",
    "        assert len(embeddings) == len(input_ids) and len(embeddings[0]) == self.embedding_matrix.shape[1]\n",
    "        return embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T08:01:14.731820Z",
     "start_time": "2020-07-24T08:01:14.728762Z"
    }
   },
   "source": [
    "First, I will create a BERT embedding matrix for unique word list which will be used in Graph models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = df[\"uniqueTokens\"].values.tolist()\n",
    "bertembeddings = BertEmbedding()\n",
    "df[\"graphEmbeddings\"] = [bertembeddings.get_embeddings(row) for row in tqdm_notebook(unique_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:29:45.921264Z",
     "start_time": "2020-08-04T02:28:22.639252Z"
    }
   },
   "outputs": [],
   "source": [
    "# # initial\n",
    "# with Pool(8) as p:\n",
    "#     unique_tokens = df[\"uniqueTokens\"].values.tolist()\n",
    "# #     MAX_LEN = int(np.median([len(row) for row in df[\"uniqueTokens\"]])) # median_unique_word_length\n",
    "# #     bertembeddings = BertEmbedding(MAX_LEN)\n",
    "#     bertembeddings = BertEmbedding()\n",
    "#     df[\"graphEmbeddings\"] = p.map(bertembeddings.get_embeddings, tqdm_notebook(unique_tokens))\n",
    "\n",
    "# # df[\"graphEmbeddings\"] = graph_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, I will also create another embedding matrix for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T02:04:41.553671Z",
     "start_time": "2020-08-04T02:04:30.031Z"
    }
   },
   "outputs": [],
   "source": [
    "with Pool(8) as p:\n",
    "    review_tokens = df[\"reviewTokens\"].values.tolist()\n",
    "#     MAX_LEN = int(np.median([len(row) for row in df[\"reviewTokens\"]])) # median_review_length\n",
    "#     bertembeddings = BertEmbedding(MAX_LEN)\n",
    "    bertembeddings = BertEmbedding()\n",
    "    language_embeddings = p.map(bertembeddings.get_embeddings, tqdm_notebook(review_tokens))\n",
    "    p.terminate()\n",
    "    \n",
    "df[\"languageEmbeddings\"] = language_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Random Embedding\n",
    "\n",
    "PyTorch will automatically generate random embeddings for us if we do not insert embeddings into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T03:04:20.500846Z",
     "start_time": "2020-08-04T03:04:09.686318Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.loc[:, [\"overall\", \"edgeIndex\", \"graphEmbeddings\"]].to_pickle(\"dataset/full_dataset/modeling_features.pkl\")\n",
    "df.loc[:, [\"overall\", \"edgeIndex\", \"graphEmbeddings\"]].to_json(args[\"modeling_feature_path\"], orient=\"columns\")\n",
    "# df.to_pickle(\n",
    "#     os.path.join(\n",
    "#         args[\"data_path\"],\n",
    "#         \"amazon_features.pkl\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
